import logging
from typing import List, Optional
from playwright.sync_api import sync_playwright, Page
from dataclasses import dataclass, asdict
import pandas as pd
import argparse
import platform
import time
import os
import random

@dataclass
class Place:
    name: str = ""
    address: str = ""
    website: str = ""
    phone_number: str = ""
    reviews_count: Optional[int] = None
    reviews_average: Optional[float] = None
    store_shopping: str = "No"
    in_store_pickup: str = "No"
    store_delivery: str = "No"
    place_type: str = ""
    opens_at: str = ""
    introduction: str = ""

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
    )

def extract_text(page: Page, xpath: str) -> str:
    try:
        if page.locator(xpath).count() > 0:
            return page.locator(xpath).inner_text()
    except Exception as e:
        logging.warning(f"Failed to extract text for xpath {xpath}: {e}")
    return ""

def extract_place(page: Page) -> Place:
    # XPaths
    name_xpath = '//div[@class="TIHn2 "]//h1[@class="DUwDvf lfPIob"]'
    address_xpath = '//button[@data-item-id="address"]//div[contains(@class, "fontBodyMedium")]'
    website_xpath = '//a[@data-item-id="authority"]//div[contains(@class, "fontBodyMedium")]'
    phone_number_xpath = '//button[contains(@data-item-id, "phone:tel:")]//div[contains(@class, "fontBodyMedium")]'
    reviews_count_xpath = '//div[@class="TIHn2 "]//div[@class="fontBodyMedium dmRWX"]//div//span//span//span[@aria-label]'
    reviews_average_xpath = '//div[@class="TIHn2 "]//div[@class="fontBodyMedium dmRWX"]//div//span[@aria-hidden]'
    info1 = '//div[@class="LTs0Rc"][1]'
    info2 = '//div[@class="LTs0Rc"][2]'
    info3 = '//div[@class="LTs0Rc"][3]'
    opens_at_xpath = '//button[contains(@data-item-id, "oh")]//div[contains(@class, "fontBodyMedium")]'
    opens_at_xpath2 = '//div[@class="MkV9"]//span[@class="ZDu9vd"]//span[2]'
    place_type_xpath = '//div[@class="LBgpqf"]//button[@class="DkEaL "]'
    intro_xpath = '//div[@class="WeS02d fontBodyMedium"]//div[@class="PYvSYb "]'

    place = Place()
    place.name = extract_text(page, name_xpath)
    place.address = extract_text(page, address_xpath)
    place.website = extract_text(page, website_xpath)
    place.phone_number = extract_text(page, phone_number_xpath)
    place.place_type = extract_text(page, place_type_xpath)
    place.introduction = extract_text(page, intro_xpath) or "None Found"

    # Reviews Count
    reviews_count_raw = extract_text(page, reviews_count_xpath)
    if reviews_count_raw:
        try:
            temp = reviews_count_raw.replace('\xa0', '').replace('(','').replace(')','').replace(',','')
            place.reviews_count = int(temp)
        except Exception as e:
            logging.warning(f"Failed to parse reviews count: {e}")
    # Reviews Average
    reviews_avg_raw = extract_text(page, reviews_average_xpath)
    if reviews_avg_raw:
        try:
            temp = reviews_avg_raw.replace(' ','').replace(',','.')
            place.reviews_average = float(temp)
        except Exception as e:
            logging.warning(f"Failed to parse reviews average: {e}")
    # Store Info
    for idx, info_xpath in enumerate([info1, info2, info3]):
        info_raw = extract_text(page, info_xpath)
        if info_raw:
            temp = info_raw.split('·')
            if len(temp) > 1:
                check = temp[1].replace("\n", "").lower()
                if 'shop' in check:
                    place.store_shopping = "Yes"
                if 'pickup' in check:
                    place.in_store_pickup = "Yes"
                if 'delivery' in check:
                    place.store_delivery = "Yes"
    # Opens At
    opens_at_raw = extract_text(page, opens_at_xpath)
    if opens_at_raw:
        opens = opens_at_raw.split('⋅')
        if len(opens) > 1:
            place.opens_at = opens[1].replace("\u202f","")
        else:
            place.opens_at = opens_at_raw.replace("\u202f","")
    else:
        opens_at2_raw = extract_text(page, opens_at_xpath2)
        if opens_at2_raw:
            opens = opens_at2_raw.split('⋅')
            if len(opens) > 1:
                place.opens_at = opens[1].replace("\u202f","")
            else:
                place.opens_at = opens_at2_raw.replace("\u202f","")
    return place

def scrape_places(search_for: str, total: int) -> List[Place]:
    setup_logging()
    places: List[Place] = []
    with sync_playwright() as p:
        # Configure browser with more stable options
        browser_args = [
            '--no-sandbox',
            '--disable-dev-shm-usage',
            '--disable-blink-features=AutomationControlled',
            '--disable-web-security',
            '--disable-features=VizDisplayCompositor'
        ]
        
        if platform.system() == "Windows":
            browser_path = r"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"
            browser = p.chromium.launch(
                executable_path=browser_path, 
                headless=False,
                args=browser_args
            )
        else:
            browser = p.chromium.launch(
                headless=False,
                args=browser_args
            )
        
        page = browser.new_page()
        
        # Set longer timeouts
        page.set_default_timeout(30000)  # 30 seconds default timeout
        
        try:
            page.goto("https://www.google.com/maps/@32.9817464,70.1930781,3.67z?", timeout=60000)
            page.wait_for_timeout(1000)
            page.locator('//input[@id="searchboxinput"]').fill(search_for)
            page.keyboard.press("Enter")
            page.wait_for_selector('//a[contains(@href, "https://www.google.com/maps/place")]')
            page.hover('//a[contains(@href, "https://www.google.com/maps/place")]')
            
            previously_counted = 0
            no_change_count = 0
            max_no_change = 8  # Increased from 5 to 8 for more persistent scrolling
            
            # Try to get at least the minimum requested results, but aim higher for buffer
            target_results = max(total * 2, 100)  # Always try to get at least 2x requested or 100, whichever is higher
            
            while True:
                # More aggressive scrolling
                page.mouse.wheel(0, 15000)  # Increased scroll distance
                page.wait_for_timeout(3000)  # Increased wait time for results to load
                page.wait_for_selector('//a[contains(@href, "https://www.google.com/maps/place")]')
                found = page.locator('//a[contains(@href, "https://www.google.com/maps/place")]').count()
                logging.info(f"Currently Found: {found} (Target: {target_results})")
                
                if found >= target_results:
                    logging.info(f"Reached target of {target_results} results")
                    break
                    
                if found == previously_counted:
                    no_change_count += 1
                    logging.info(f"No new results found, attempt {no_change_count}/{max_no_change}")
                    if no_change_count >= max_no_change:
                        logging.info("Reached maximum attempts with no new results")
                        break
                    # Try different scrolling pattern when stuck
                    page.mouse.wheel(0, 20000)
                    page.wait_for_timeout(2000)
                    page.mouse.wheel(0, -5000)  # Scroll back up a bit
                    page.wait_for_timeout(1000)
                else:
                    no_change_count = 0  # Reset counter if we found new results
                    
                previously_counted = found
                
                # Add some randomized scrolling to avoid detection
                random_scroll = random.randint(8000, 12000)
                page.mouse.wheel(0, random_scroll)
                page.wait_for_timeout(random.randint(1000, 2000))
                
            # Get all available listings, but ensure we have enough
            all_listings = page.locator('//a[contains(@href, "https://www.google.com/maps/place")]').all()
            
            # Process ALL available listings to maximize results
            listings = [listing.locator("xpath=..") for listing in all_listings]
            
            logging.info(f"Total Found: {len(listings)}, Processing all to ensure minimum {total} valid results")
            
            for idx, listing in enumerate(listings):
                try:
                    # Add more robust clicking and waiting
                    page.wait_for_timeout(500)  # Small delay before clicking
                    listing.click()
                    
                    # Wait for the place details to load with multiple fallbacks
                    try:
                        page.wait_for_selector('//div[@class="TIHn2 "]//h1[@class="DUwDvf lfPIob"]', timeout=15000)
                    except:
                        # Try alternative selector if first one fails
                        try:
                            page.wait_for_selector('//h1[contains(@class, "DUwDvf")]', timeout=5000)
                        except:
                            logging.warning(f"Could not load details for listing {idx+1}, skipping")
                            continue
                    
                    time.sleep(3)  # Give more time for all details to load
                    place = extract_place(page)
                    if place.name:
                        places.append(place)
                        logging.info(f"Extracted place {len(places)}: {place.name}")
                        # Only stop if we have significantly more than requested to ensure we get enough
                        # Remove the early stopping condition that was causing the issue
                        if len(places) >= total * 1.5:  # Only stop if we have 50% more than requested
                            logging.info(f"Exceeded target significantly: {len(places)} >= {total * 1.5}")
                            break
                    else:
                        logging.warning(f"No name found for listing {idx+1}, skipping.")
                except Exception as e:
                    logging.warning(f"Failed to extract listing {idx+1}: {e}")
                    # Try to recover by waiting and continuing
                    page.wait_for_timeout(1000)
                    continue
                    
        finally:
            browser.close()
    
    logging.info(f"Final result: {len(places)} places extracted")
    return places

def save_places_to_csv(places: List[Place], output_path: str = "result.csv", append: bool = False):
    df = pd.DataFrame([asdict(place) for place in places])
    if not df.empty:
        for column in df.columns:
            if df[column].nunique() == 1:
                df.drop(column, axis=1, inplace=True)
        file_exists = os.path.isfile(output_path)
        mode = "a" if append else "w"
        header = not (append and file_exists)
        df.to_csv(output_path, index=False, mode=mode, header=header)
        logging.info(f"Saved {len(df)} places to {output_path} (append={append})")
    else:
        logging.warning("No data to save. DataFrame is empty.")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--search", type=str, help="Search query for Google Maps")
    parser.add_argument("-t", "--total", type=int, help="Minimum number of results to scrape")
    parser.add_argument("-o", "--output", type=str, default="result.csv", help="Output CSV file path")
    parser.add_argument("--append", action="store_true", help="Append results to the output file instead of overwriting")
    args = parser.parse_args()
    search_for = args.search or "turkish stores in toronto Canada"
    total = args.total or 20  # Increased default minimum
    output_path = args.output
    append = args.append
    places = scrape_places(search_for, total)
    save_places_to_csv(places, output_path, append=append)

if __name__ == "__main__":
    main()
